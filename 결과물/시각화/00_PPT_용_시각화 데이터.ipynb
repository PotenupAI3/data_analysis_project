{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b7bbe7",
   "metadata": {},
   "source": [
    "# ** 실행 방법 **\n",
    "```\n",
    "uv pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119049c",
   "metadata": {},
   "source": [
    "# 1. 워드 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c84d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "stopwords = [\n",
    "    \"진짜\",\n",
    "    \"정말\",\n",
    "    \"영상\",\n",
    "    \"채널\",\n",
    "    \"구독\",\n",
    "    \"좋아요\",\n",
    "    \"댓글\",\n",
    "    \"보고\",\n",
    "    \"수\",\n",
    "    \"거\",\n",
    "    \"나\",\n",
    "    \"그럼\",\n",
    "    \"그런\",\n",
    "    \"근데\",\n",
    "    \"그런데\",\n",
    "    \"이런\",\n",
    "    \"저런\",\n",
    "    \"것\",\n",
    "    \"수\",\n",
    "    \"등\",\n",
    "    \"좀\",\n",
    "    \"진짜\",\n",
    "    \"ㅋㅋ\",\n",
    "    \"ㅋㅋㅋ\",\n",
    "    \"ㅋㅋㅋㅋ\",\n",
    "    \"ㅎㅎ\",\n",
    "    \"ㅎㅎㅎ\",\n",
    "    \"아냐\",\n",
    "    \"아니\",\n",
    "    \"뭐\",\n",
    "    \"왜\",\n",
    "    \"하고\",\n",
    "    \"하는\",\n",
    "    \"하다\",\n",
    "    \"됐다\",\n",
    "    \"되다\",\n",
    "    \"있다\",\n",
    "    \"없다\",\n",
    "    \"때\",\n",
    "    \"때문\",\n",
    "    \"이유\",\n",
    "    \"전주\",\n",
    "    \"익산\",  # <- 특정 단어가 너무 흔해서 규칙을 망치면 넣어도 됨(선택)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../결과물/시각화/comments.csv\")\n",
    "texts = df[\"texts\"].tolist()\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.youtube import collect_all_comments\n",
    "\n",
    "rows, texts = collect_all_comments(\"FVFRVGSvkq0\")\n",
    "\n",
    "print(len(rows), len(texts))\n",
    "print(rows[:100], texts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.DataFrame({\"texts\": texts})\n",
    "csv.to_csv(\"./시각화_결과물/comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.visualization import word_cloud\n",
    "\n",
    "word_cloud(texts, stopwords=stopwords, mask_img_path=\"../../images/circle.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0efc",
   "metadata": {},
   "source": [
    "# 2. 장바구니 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_ko(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^0-9a-zA-Z가-힣\\s]\", \" \", s)  # 특수문자 제거\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "# 최소한의 불용어(필요하면 계속 추가)\n",
    "STOP = {\n",
    "    \"그럼\",\n",
    "    \"그런\",\n",
    "    \"근데\",\n",
    "    \"그런데\",\n",
    "    \"이런\",\n",
    "    \"저런\",\n",
    "    \"것\",\n",
    "    \"수\",\n",
    "    \"등\",\n",
    "    \"좀\",\n",
    "    \"진짜\",\n",
    "    \"ㅋㅋ\",\n",
    "    \"ㅋㅋㅋ\",\n",
    "    \"ㅋㅋㅋㅋ\",\n",
    "    \"ㅎㅎ\",\n",
    "    \"ㅎㅎㅎ\",\n",
    "    \"아냐\",\n",
    "    \"아니\",\n",
    "    \"뭐\",\n",
    "    \"왜\",\n",
    "    \"하고\",\n",
    "    \"하는\",\n",
    "    \"하다\",\n",
    "    \"됐다\",\n",
    "    \"되다\",\n",
    "    \"있다\",\n",
    "    \"없다\",\n",
    "    \"때\",\n",
    "    \"때문\",\n",
    "    \"이유\",\n",
    "    \"전주\",\n",
    "    \"익산\",  # <- 특정 단어가 너무 흔해서 규칙을 망치면 넣어도 됨(선택)\n",
    "}\n",
    "\n",
    "\n",
    "def text_to_items(text: str, min_len: int = 2) -> list[str]:\n",
    "    t = clean_ko(text)\n",
    "    toks = [w for w in t.split() if len(w) >= min_len]\n",
    "    toks = [w for w in toks if w not in STOP]\n",
    "    return toks\n",
    "\n",
    "\n",
    "items_list = [text_to_items(t) for t in texts]\n",
    "items_list[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# STOP_NOUN = {\"전북\",\"전주\",\"익산\",\"군산\",\"완주\",\"김제\"}  # 너무 흔하면 제외(선택)\n",
    "STOP_NOUN = []\n",
    "\n",
    "\n",
    "def nouns_kiwi(text: str) -> list[str]:\n",
    "    text = clean_ko(text)\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    nouns = [t.form for t in tokens if t.tag.startswith(\"NN\")]  # 명사류\n",
    "    nouns = [n for n in nouns if len(n) >= 2 and n not in STOP_NOUN]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "items_list = [nouns_kiwi(t) for t in texts]\n",
    "df_items = pd.DataFrame({\"items\": items_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f071b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_arr = te.fit(items_list).transform(items_list)\n",
    "te_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6478e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 문장에 각각의 단어들이 있는지 없는지를 True / False\n",
    "df = pd.DataFrame(te_arr, columns=te.columns_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# support는 전체 문장 중에 itemset이 등장한 문장의 비율\n",
    "# min_support : support가 min_support 이상인것만 보여주세요\n",
    "# max_len : max_len개의 조합까지 보여주세요\n",
    "frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True, max_len=2)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_data = rules.head(20).pivot_table(\n",
    "    index=\"antecedents\",  # 행\n",
    "    columns=\"consequents\",  # 열\n",
    "    values=\"lift\",  # 기준\n",
    "    fill_value=0,  # 매칭되지 않는 것은 이걸로 채워라.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.analysis import market_basket_ko\n",
    "\n",
    "result = market_basket_ko(\n",
    "    texts,\n",
    "    min_support=0.02,\n",
    "    max_len=2,\n",
    "    top_k=20,\n",
    "    stop_noun=[\"전북\", \"전주\", \"익산\", \"군산\", \"완주\", \"김제\"],  # 필요하면\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5430ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib  # noqa: F401\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    result[\"pivot_data\"],\n",
    "    annot=True,\n",
    "    cmap=\"YlGnBu\",\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.3,\n",
    "    square=True,\n",
    ")\n",
    "plt.title(\"연관성 분석 시각화(Lift 기준)\")\n",
    "plt.xlabel(\"Consequents\")\n",
    "plt.ylabel(\"Antecedents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib  # noqa: F401, F811\n",
    "\n",
    "rules = result[\"rules\"]\n",
    "my_rules = rules.sort_values(by=[\"lift\"], ascending=False).head(50)\n",
    "\n",
    "# 1. 그래프 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# 2. 엣지 추가\n",
    "for _, col_data in my_rules.iterrows():\n",
    "    # 1) 단어 추출\n",
    "    print(f\"[BEFORE] {col_data['antecedents']}, {col_data['consequents']}\")\n",
    "    antecedent = \",\".join(col_data[\"antecedents\"])\n",
    "    consequent = \",\".join(col_data[\"consequents\"])\n",
    "    print(f\"[AFTER] {antecedent}, {consequent}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # 2) 지표 추출\n",
    "    weight = col_data[\"lift\"]\n",
    "\n",
    "    # 3) 그래프에 정보 추가\n",
    "    G.add_edge(antecedent, consequent, weight=weight)\n",
    "\n",
    "G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 노드 배치\n",
    "# position = nx.kamada_kawai_layout(G, scale=0.5)\n",
    "# k를 조절하면 노드간 거리를 조절할 수 있습니다.\n",
    "position = nx.spring_layout(G, k=0.9, seed=15)\n",
    "\n",
    "# 4. 가중치 추출\n",
    "scale = 0.3  ## 선의 굵기가 너무 굵다면 사이즈를 줄일 수 있습니다.\n",
    "edge_weights = [G[u][v][\"weight\"] * scale for u, v in G.edges()]\n",
    "print(edge_weights)\n",
    "\n",
    "# 5. 그리기\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_nodes(G, position, node_color=\"lightblue\", node_size=1000)\n",
    "nx.draw_networkx_edges(G, position, edge_color=\"gray\", width=edge_weights)\n",
    "nx.draw_networkx_labels(G, position, font_size=10, font_family=\"Malgun Gothic\")\n",
    "plt.title(\"단어 간 연관규칙 기반 네트워크 그래프(Lift 기준)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_project (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
