{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "part = \"snippet\"\n",
    "videoId = \"FVFRVGSvkq0\"\n",
    "key = \"AIzaSyCv2c8L4r2g__tHz4aeqLPzOiDGiBO9yCw\"\n",
    "maxResults = 100\n",
    "textFormat = \"plainText\"\n",
    "nextPageToken = None \n",
    "\n",
    "data_list = []\n",
    "max_rep = 26\n",
    "\n",
    "for i in range(max_rep):\n",
    "    print(f\"{i+1} 번째 데이터 수집을 시작합니다.\")\n",
    "    if nextPageToken is not None:\n",
    "        url = f\"{base_url}?part={part}&videoId={videoId}&key={key}&maxResults={maxResults}&textFormat={textFormat}&pageToken={nextPageToken}\"\n",
    "    else:\n",
    "        url = f\"{base_url}?part={part}&videoId={videoId}&key={key}&maxResults={maxResults}&textFormat={textFormat}\"\n",
    "    print(url)\n",
    "    # 3. API 요청\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        nextPageToken = data.get(\"nextPageToken\")\n",
    "\n",
    "        # 6. 데이터를 수집한다. \n",
    "        items = data[\"items\"]\n",
    "        item_list = []\n",
    "        for item in items:\n",
    "            item_list.append(item[\"snippet\"][\"topLevelComment\"][\"snippet\"])\n",
    "        \n",
    "        # print(f\"\\t{len(item_list)} 개의 데이터를 수집했습니다.\")\n",
    "        \n",
    "        # 7. data_list에 담는다. \n",
    "        data_list.extend(item_list)\n",
    "\n",
    "        # print(nextPageToken)\n",
    "        # display(pd.DataFrame(item_list))\n",
    "        if nextPageToken is None:\n",
    "            print(\"다음 페이지가 없어 종료합니다.\")\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a166d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"textOriginal\": \"text\"})\n",
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP1. 전처리한 문장을 담을 빈리스트를 준비한다. sent_list \n",
    "sent_list = []\n",
    "\n",
    "# STEP2. df[\"text\"]에서 문장을 하나씩 뺀다. sent \n",
    "for sent in df[\"text\"]:\n",
    "    # STEP3. 문장을 정규표현식을 이용해서 전처리한다. clean_sent\n",
    "    print(\"STEP3. 문장을 전처리합니다.\")\n",
    "    clean_sent = re.sub(\"[^0-9a-zA-Z가-힣\\s]\", \"\", sent)\n",
    "    # STEP4. clean_sent를 토크나이징한다. (Kiwi 형태소 분석기 사용) result\n",
    "    print(\"STEP4. 문장을 토크나이징합니다.\")\n",
    "    result = kiwi.tokenize(clean_sent)\n",
    "    print(\"\\tsub_list를 생성합니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for x in result:\n",
    "        # 1) 조건 : 조사(J), 어미(E), 접미사(X)는 포함하지 않는다. -> pos[0] in [\"J\", \"E\", \"X\"] 건너뛰기\n",
    "        word = x.form\n",
    "        pos = x.tag \n",
    "        if pos[0] in [\"J\", \"E\", \"X\"]:\n",
    "            # print(f\"\\t건너뛰기!! word={word} pos={pos}\")\n",
    "            continue\n",
    "        # 2) 조건 : 한 글자인 단어는 포함하지 않는다.\n",
    "        if len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"\\t추가!! word={word} pos={pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # STEP5. result를 \" \"를 구분자로 하나의 문자열로 합친다. result_str\n",
    "    result_str = \" \".join(sub_list)\n",
    "    print(f\"[RESULT_STR] {result_str}\")\n",
    "    # STEP6. result_str을 sent_list에 넣는다.\n",
    "    sent_list.append(result_str)\n",
    "    print(sent)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05081e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df = 0.1,       # 전체 단어의 등장 비율이 p이상인 것만 사용\n",
    "    min_df=2,           # 이 단어가 적어도 n 개 이상 있는 것만 사용\n",
    "    max_features=1000,  # 최대 몇 개까지 나타낼 것인가\n",
    "    ngram_range=(1,2)   # 단어의 조합 설정(ex. 1개만 사용)\n",
    ")\n",
    "\n",
    "feat_vec = count_vectorizer.fit_transform(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc55b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Sparse Matrix를 일반 배열(Dense)로 변환: .toarray()\n",
    "# 2. 열 이름(단어 목록) 가져오기: get_feature_names_out()\n",
    "df_vec = pd.DataFrame(\n",
    "    feat_vec.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5) # 몇 개의 topic으로 할까요?\n",
    "lda.fit(feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e88a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.lda_model.prepare(lda, feat_vec, count_vectorizer)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric이 min_threshold 이상인 것만 보여주세요.\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8275b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table\n",
    "pivot_data = rules.head(20).pivot_table(\n",
    "    index=\"antecedents\",        # 행\n",
    "    columns=\"consequents\",      # 열\n",
    "    values=\"lift\",              # 기준\n",
    "    fill_value=0                # 매칭되지 않는 것은 이걸로 채워라.\n",
    ")\n",
    "pivot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(pivot_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.3, square=True)\n",
    "plt.title(\"연관성 분석 시각화(Lift 기준)\")\n",
    "plt.xlabel(\"Consequents\")\n",
    "plt.ylabel(\"Antecedents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78250248",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[[\"antecedents\",\"consequents\",\"support\",\"confidence\", \"lift\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterrows 이해하기\n",
    "# 데이터프레임에서 하나의 행씩 추출 (인덱스, 열 데이터)\n",
    "sample_data = rules[[\"antecedents\", \"consequents\"]].head(2)\n",
    "for x in sample_data.iterrows():\n",
    "    print(f\"x의 요소 개수: {len(x)}\")\n",
    "    print(x[0])\n",
    "    print(\"-\"*50)\n",
    "    print(x[1])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae766c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 보여주고 싶은 데이터 설정하기(여러분이 설정하세요.)\n",
    "my_rules = rules.sort_values(by=[\"lift\"], ascending=False).head(50)\n",
    "\n",
    "# 1. 그래프 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# 2. 엣지 추가\n",
    "for _ , col_data in my_rules.iterrows():\n",
    "    # 1) 단어 추출\n",
    "    print(f\"[BEFORE] {col_data['antecedents']}, {col_data['consequents']}\")\n",
    "    antecedent = \",\".join(col_data[\"antecedents\"])\n",
    "    consequent = \",\".join(col_data[\"consequents\"])\n",
    "    print(f\"[AFTER] {antecedent}, {consequent}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # 2) 지표 추출\n",
    "    weight = col_data[\"lift\"]\n",
    "\n",
    "    # 3) 그래프에 정보 추가\n",
    "    G.add_edge(antecedent, consequent, weight=weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 노드 배치\n",
    "# position = nx.kamada_kawai_layout(G, scale=0.5)\n",
    "# k를 조절하면 노드간 거리를 조절할 수 있습니다.\n",
    "position = nx.spring_layout(G, k=0.9, seed=15)\n",
    "\n",
    "# 4. 가중치 추출\n",
    "scale = 0.3 ## 선의 굵기가 너무 굵다면 사이즈를 줄일 수 있습니다.\n",
    "edge_weights = [G[u][v][\"weight\"]*scale for u, v in G.edges()]\n",
    "print(edge_weights)\n",
    "\n",
    "# 5. 그리기\n",
    "plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx_nodes(G, position, node_color=\"lightblue\", node_size=1000)\n",
    "nx.draw_networkx_edges(G, position, edge_color=\"gray\", width=edge_weights)\n",
    "nx.draw_networkx_labels(G, position, font_size=10, font_family=\"Malgun Gothic\")\n",
    "plt.title(\"단어 간 연관규칙 기반 네트워크 그래프(Lift 기준)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176350d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
